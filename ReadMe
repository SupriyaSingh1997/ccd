## 1. Understanding the Problem Statement: 

The goal of this study is to identify credit card users who are more likely to experience a payment default in the upcoming month. 
Many organisations that issue credit cards are developing predictive models that will enable them to anticipate a customer's payment status using the customer's credit score, credit history, payment history, and other data. The goal of this research is to estimate the likelihood that a specific client would go into default the following month utilising personal and financial data about the consumer, such as credit line, age, repayment history, and delinquency history for the previous six months. To create a binary predictive model, numerous statistical and data mining approaches will be employed. It will be easier for credit card issuing corporations to pursue specific clients, make strategic efforts to avert defaults, and effectively offset future losses if they are able to accurately identify the impending default of customers in advance. In any case, the material does not explicitly identify any specific person or offer details that could be used to decrypt a connection to a specific person. The goal of this study is to forecast the likelihood that credit card users will become indebted in the following month using payment information from October 2015 to March 2016. Out of the 30,000 observations overall,The binary variable for default payment in April 2016 (Yes = 1, No = 0) must be determined.

2. Data Pre-processing: 

We must carry out fundamental data pre-processing tasks like null value imputation and the removal of undesirable data before moving on to the exploration stage.
2.1. Libraries Used 
Pandas ,Numpy , Matplolib, Seaborn,Sklearn

The dataset contains 30000 rows and 25 columns. But we only need information about important columns. and dropping other unwanted columns.

2.2. Handling Null Values:
The dataset contains a small number of null values. The majority of the dataset contains null values, which causes several processing bottlenecks. As a result, it is necessary to identify the columns that include lump sum null values and get them discarded. Therefore, let's remove these null values columns that have more than 80% null values.

